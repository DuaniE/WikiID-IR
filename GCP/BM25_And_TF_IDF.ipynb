{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB4VMR0xs_l2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict,Counter\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from itertools import islice,count\n",
        "from contextlib import closing\n",
        "\n",
        "import json\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "from itertools import chain\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation remarks:\n",
        "* TF-IDF: use [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To deal with stopwords use the argument `stop_words`\n",
        "\n",
        "TfidfVectorizer suggests handling with additional parameters, as you can read in the documentation. Make sure you read about them.\n",
        "\n",
        "* Cosine Similarity: use [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html).\n",
        "\n",
        "* BM25: Implement a BM25 version according to the provided skeleton **without the use of packages**. 15 points as follows:\n",
        "    * Prepare the data and filter stopwords. (5 points)\n",
        "    * Implement two functions at BM25 class. (10 points)\n",
        "\n",
        "* Ranking: implement topN functionallity (10 points)\n",
        "\n",
        "**Later in this assignment, we will write code for TF-IDF and BM25 that utilize the inverted index.**"
      ],
      "metadata": {
        "id": "FBNqnOAJwV9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when working with BM25 without any package we need to filter the data and clean it by ourselves, We will do so utilizing NLTK stopwords."
      ],
      "metadata": {
        "id": "LOFtpwpxzZq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stopwords_frozen = frozenset(stopwords.words('english'))\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
        "    return list_of_tokens\n",
        "\n",
        "\n",
        "# clean_data = [tokenize(doc) for doc in data]\n",
        "# clean_data"
      ],
      "metadata": {
        "id": "P2SWyaKX4ffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class below is a BM25 class but without any DL (documents length) this will help up with the calculation on the BM25 class (like calculate easily the AVDL).\n",
        "\n",
        "second reason is that in the other class (**BM25_from_index**) we have improvments like when we compare queries to documents we can search just only on the documets that is represented in the posting list of the term and not on all the documents in the corpus every time (like the class BM25 below this cell)"
      ],
      "metadata": {
        "id": "MCv0ivTHq0HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25:\n",
        "    \"\"\"\n",
        "    Best Match 25.\n",
        "\n",
        "    Parameters to tune\n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tf_ : list[dict[str, int]]\n",
        "        Term Frequency per document. So [{'hi': 1}] means\n",
        "        the first document contains the term 'hi' 1 time.\n",
        "        The frequnecy is normilzied by the max term frequency for each document.\n",
        "\n",
        "    doc_len_ : list[int]\n",
        "        Number of terms per document. So [3] means the first\n",
        "        document contains 3 terms. \n",
        "        \n",
        "    df_ : dict[str, int]\n",
        "        Document Frequency per term. i.e. Number of documents in the\n",
        "        corpus that contains the term.       \n",
        "\n",
        "    avg_doc_len_ : float\n",
        "        Average number of terms for documents in the corpus.\n",
        "\n",
        "    idf_ : dict[str, float]\n",
        "        Inverse Document Frequency per term.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,doc_len,df,tf=None,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.tf_ = tf\n",
        "        self.doc_len_ = doc_len\n",
        "        self.df_ = df\n",
        "        self.N_ = len(doc_len)\n",
        "        self.avgdl_ = sum(doc_len) / len(doc_len)        \n",
        "        \n",
        "\n",
        "    def calc_idf(self,query):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"\n",
        "        #idf =  ln((Nâˆ’n(ti)+0.5n(ti)+0.5)+1) , where  N  is the total number of documents in the collection, and  n(ti)  is the number of documents containing  ti  (e.g., document frequency (df)).\n",
        "        N = self.N_\n",
        "        df = self.df_\n",
        "        idf = {}\n",
        "        for term in query:\n",
        "          if term not in df:\n",
        "            df_term = 0\n",
        "          else:\n",
        "            df_term = df[term]\n",
        "          numerator = N - df_term + 0.5\n",
        "          denominator = df_term + 0.5\n",
        "          score = math.log((numerator / denominator) + 1)\n",
        "          if term not in idf:\n",
        "            idf[term] = score\n",
        "          else:\n",
        "            idf[term] += score\n",
        "        return idf\n",
        "        \n",
        "\n",
        "    def search(self, queries):\n",
        "        \"\"\"\n",
        "        This function use the _score function to calculate the bm25 score for all queries provided.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        queries: list of lists. Each inner list is a list of tokens. For example:\n",
        "                                                                                    [\n",
        "                                                                                        ['look', 'blue', 'sky'],\n",
        "                                                                                        ['likes', 'blue', 'sun'],\n",
        "                                                                                        ['likes', 'diamonds']\n",
        "                                                                                    ]\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "        list of scores of bm25\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for query in queries:            \n",
        "            scores.append([self._score(query, doc_id) for doc_id in range(self.N_)])\n",
        "        return scores\n",
        "    \n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        B_Normalizer = 1- self.b + self.b * (self.doc_len_[doc_id] / self.avgdl_)\n",
        "        doc_tf = self.tf_[doc_id]\n",
        "        total_idf_dict = self.calc_idf(query)\n",
        "        sui = 0\n",
        "        for term in query:\n",
        "          if term in doc_tf:\n",
        "            tf_of_term_per_doc = doc_tf[term]\n",
        "            numerator_tf_star = (self.k1 + 1) * tf_of_term_per_doc\n",
        "            denominator_tf_star = (B_Normalizer * self.k1) + tf_of_term_per_doc\n",
        "            score_per_term = (numerator_tf_star / denominator_tf_star) * total_idf_dict[term]\n",
        "            sui += score_per_term\n",
        "        return sui if sui > 0 else 0 "
      ],
      "metadata": {
        "id": "V_NUsBch8kCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this below cell is for tests! later on we will reallocate this to the test class"
      ],
      "metadata": {
        "id": "RfkwEPCEzgHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Remove tokenizing and remove stopwords from queries\n",
        "# clean_queries = [tokenize(query) for query in queries]\n",
        "# # now we can search for results to our queries\n",
        "# BM25_res = bm25.search(clean_queries)\n",
        "# # data frame that represent the queries (as rows) on documents (as columns) \n",
        "# BM25_df = pd.DataFrame(data = BM25_res,index = [query_id for query_id in range(len(clean_queries))] ,columns = [doc_id for doc_id in range(len(data))])"
      ],
      "metadata": {
        "id": "jFoD5aKfDb6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The class of BM25**"
      ],
      "metadata": {
        "id": "CWaHfydD8gAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cells represent configuration before running the test! for later on we need to reallocate them"
      ],
      "metadata": {
        "id": "4f5N645Now3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is BM25 preprocessing data**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Which mean that here we take a list of strings(could be the body_text, the title_text or the anchor_text) and **return three objects as follows**:\n",
        "\n",
        "a) doc_len: list of integer. Each element represents the length of a document.\n",
        "\n",
        "b) tf: list of dictionaries. Each dictionary corresponds to a document as follows:\n",
        "\n",
        "- key: term\n",
        "- value: normalized term frequency (by the length of document)\n",
        "\n",
        "for example, tf -> [{Burekas: 3, Haagala:2}, {Mi:3, Shemekir:1}, {Mekir:3, Burekas: 1, Haagala:4}] (each index in the list represent a doc)\n",
        "\n",
        "c) df: dictionary representing the document frequency as follows:\n",
        "- key: term  \n",
        "- value: document frequency\n",
        "\n",
        "for example, df -> {Burekas: 4, Haagala:6, Mi:3, Shemekir:1, Mekir:3}"
      ],
      "metadata": {
        "id": "e4db4Ggx5Jq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_preprocess(data):\n",
        "    \"\"\"\n",
        "    This function goes through the data and saves relevant information for the calculation of bm25. \n",
        "    Specifically, in this function, we will create 3 objects that gather information regarding document length, term frequency and\n",
        "    document frequency.\n",
        "    Parameters\n",
        "    -----------\n",
        "    data: list of lists. Each inner list is a list of tokens. \n",
        "    Example of data: \n",
        "    [\n",
        "        ['sky', 'blue', 'see', 'blue', 'sun'],\n",
        "        ['sun', 'bright', 'yellow'],\n",
        "        ['comes', 'blue', 'sun'],\n",
        "        ['lucy', 'sky', 'diamonds', 'see', 'sun', 'sky'],\n",
        "        ['sun', 'sun', 'blue', 'sun'],\n",
        "        ['lucy', 'likes', 'blue', 'bright', 'diamonds']\n",
        "    ]\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    three objects as follows:\n",
        "                a) doc_len: list of integer. Each element represents the length of a document.\n",
        "                b) tf: list of dictionaries. Each dictionary corresponds to a document as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: normalized term frequency (by the length of document)\n",
        "\n",
        "                                                                                               \n",
        "                c) df: dictionary representing the document frequency as follows:\n",
        "                                                                    key: term\n",
        "                                                                    value: document frequency\n",
        "    \"\"\"      \n",
        "    doc_len = []\n",
        "    tf = []\n",
        "    df = {}\n",
        "    for elem in data:\n",
        "      #for calculation of doc_len\n",
        "      doc_len.append(len(elem))\n",
        "      frequency = dict(Counter(elem))\n",
        "      #normalize the tf for the len of the document\n",
        "      for key in frequency:\n",
        "        frequency[key] /= len(elem)\n",
        "        if key not in df:\n",
        "          df[key] = 1\n",
        "        else:\n",
        "          df[key] += 1\n",
        "      #for calculation of tf\n",
        "      tf.append(frequency)\n",
        "    return doc_len,tf,df\n",
        "# doc_len,tf,df = bm25_preprocess(clean_data)"
      ],
      "metadata": {
        "id": "T22ukC8z5IVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this assignment, we are experimenting with two methods. ***TF-IDF*** and ***BM25***.  with a given query or queries we'll return a ranked list of documents to retrieve."
      ],
      "metadata": {
        "id": "TpBtWauvxURJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swC-nUdwviXN"
      },
      "source": [
        "#### BM25 for carnfield data (15 points)\n",
        "As a reminder:\n",
        "\n",
        "To use BM25 we will need to following parameters:\n",
        "\n",
        "* $k1$ and $b$ - free parameters\n",
        "* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$\n",
        "* |$D$|- is the length of the document $D$ in terms \n",
        "* $avgdl$ -  average document length\n",
        "* $IDF$ - which is calculted as follows: $ln(\\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df)).\n",
        "\n",
        "Now, we will use the inverted index to fetch this information.\n",
        "\n",
        "**We need to check only documents which are 'candidates' for a given query.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "nPOS5HoohGTJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d622e6d162d317bde0c016e385409d48",
          "grade": false,
          "grade_id": "cell-e55a7a9f6855d2a6",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from itertools import chain\n",
        "import time\n",
        "# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n",
        "class BM25_from_index:\n",
        "    \"\"\"\n",
        "    Best Match 25.    \n",
        "    ----------\n",
        "    k1 : float, default 1.5\n",
        "\n",
        "    b : float, default 0.75\n",
        "\n",
        "    index: inverted index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,index,k1=1.5, b=0.75):\n",
        "        self.b = b\n",
        "        self.k1 = k1\n",
        "        self.index = index\n",
        "        self.N = len(DL)\n",
        "        self.AVGDL = sum(DL.values())/self.N\n",
        "        self.words, self.pls = zip(*self.index.posting_lists_iter())        \n",
        "\n",
        "    def calc_idf(self,list_of_tokens):\n",
        "        \"\"\"\n",
        "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        idf: dictionary of idf scores. As follows: \n",
        "                                                    key: term\n",
        "                                                    value: bm25 idf score\n",
        "        \"\"\"        \n",
        "        idf = {}        \n",
        "        for term in list_of_tokens:            \n",
        "            if term in self.index.df.keys():\n",
        "                n_ti = self.index.df[term]\n",
        "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
        "            else:\n",
        "                pass                             \n",
        "        return idf\n",
        "        \n",
        "\n",
        "    def search(self, queries,N=3):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        We need to check only documents which are 'candidates' for a given query. \n",
        "        This function return a dictionary of scores as the following:\n",
        "                                                                    key: query_id\n",
        "                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        a_scorer = {}\n",
        "        top_N_scores = get_topN_score_for_queries(queries,self.index,N)\n",
        "        for query_num, doc_id_scores in top_N_scores.items():\n",
        "          cand_docs = get_candidate_documents_and_scores(queries[query_num],self.index,self.words,self.pls)\n",
        "          cand_docs_id = [num for num, term in cand_docs.keys()]\n",
        "          a_scorer[query_num] = list()\n",
        "          for doc_id_sc in doc_id_scores:\n",
        "            if doc_id_sc[0] in cand_docs_id:\n",
        "              doc_id = doc_id_sc[0]\n",
        "              score = self._score(queries[query_num], doc_id)\n",
        "              if query_num in a_scorer:\n",
        "                a_scorer[query_num].append((doc_id, score))\n",
        "              else:\n",
        "                a_scorer[query_num].append((doc_id, score))\n",
        "          a_scorer[query_num].sort(key= lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "        return a_scorer\n",
        "\n",
        "    def _score(self, query, doc_id):\n",
        "        \"\"\"\n",
        "        This function calculate the bm25 score for given query and document.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
        "        doc_id: integer, document id.\n",
        "        \n",
        "        Returns:\n",
        "        -----------\n",
        "        score: float, bm25 score.\n",
        "        \"\"\"        \n",
        "        score = 0.0        \n",
        "        doc_len = DL[str(doc_id)]        \n",
        "        \n",
        "        self.idf = self.calc_idf(query)\n",
        "\n",
        "        for term in query:\n",
        "            if term in self.index.term_total.keys():                \n",
        "                \n",
        "                term_frequencies = dict(self.pls[self.words.index(term)])                \n",
        "                if doc_id in term_frequencies.keys():            \n",
        "                    freq = term_frequencies[doc_id]\n",
        "                    numerator = self.idf[term] * freq * (self.k1 + 1)\n",
        "                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
        "                    score += (numerator / denominator)\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cells represent tests! for later on we need to reallocate them"
      ],
      "metadata": {
        "id": "TAd75vLAonPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idx_title = InvertedIndex.read_index('title_index', 'title')\n",
        "# idx_body = InvertedIndex.read_index('body_index', 'body')\n",
        "# # read posting lists from disk\n",
        "# words, pls = zip(*idx_title.posting_lists_iter())"
      ],
      "metadata": {
        "id": "Z6K_PfHmomVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51fbd11cd4366bc937dbfdb776e1d39a",
          "grade": false,
          "grade_id": "cell-091c53ed5e95592e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hMqCUfHjXk-7"
      },
      "outputs": [],
      "source": [
        "# bm25_title = BM25_from_index(idx_title)\n",
        "# bm25_queries_score_train = bm25_title.search(cran_txt_query_text_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBlLqQii5f6B"
      },
      "source": [
        "#### TF-IDF for carnfield data \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qulheONwg8"
      },
      "source": [
        "Bellow cells contain the following functions: \n",
        "\n",
        "*   *generate_query_tfidf_vector* - Generate a vector representing the query\n",
        "*   *get_candidate_documents_and_scores* - Generate a dictionary representing a pool of candidate documents for a given query.\n",
        "*   *generate_document_tfidf_matrix* - Generate a DataFrame of tfidf scores for a given query.\n",
        "*   *cosine_similarity* - Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "\n",
        "*   *get_top_n* - Sort and return the highest N documents according to the cosine similarity score.\n",
        "\n",
        "*   *get_topN_score_for_queries* - Generate a dictionary that gather for every query its topN score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mbPJ5YxcXRcT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7aedea7913856a9082c9c0bc9a338f0c",
          "grade": false,
          "grade_id": "cell-e244d9f201109308",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_query_tfidf_vector(query_to_search,index):\n",
        "    \"\"\" \n",
        "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
        "    The terms representing the query will be the unique terms in the index.\n",
        "\n",
        "    We will use tfidf on the query as well. \n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the query.    \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    vectorized query with tfidf scores\n",
        "    \"\"\"\n",
        "    \n",
        "    epsilon = .0000001\n",
        "    total_vocab_size = len(index.term_total)\n",
        "    Q = np.zeros((total_vocab_size))\n",
        "    term_vector = list(index.term_total.keys())    \n",
        "    counter = Counter(query_to_search)\n",
        "    for token in np.unique(query_to_search):\n",
        "        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               \n",
        "            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query\n",
        "            df = index.df[token]            \n",
        "            idf = math.log((len(DL))/(df+epsilon),10) #smoothing\n",
        "            \n",
        "            try:\n",
        "                ind = term_vector.index(token)\n",
        "                Q[ind] = tf*idf                    \n",
        "            except:\n",
        "                pass\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Hwf5oq558hNh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "13ba9f2e92bb01e6ab6e71368ce3237a",
          "grade": false,
          "grade_id": "cell-f6349be47be84f49",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_posting_iter(index):\n",
        "    \"\"\"\n",
        "    This function returning the iterator working with posting list.\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    index: inverted index    \n",
        "    \"\"\"\n",
        "    words, pls = zip(*index.posting_lists_iter())\n",
        "    return words,pls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HhQRw9ye10r1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fcbd55876e4aab5ad2954edd94f0de80",
          "grade": false,
          "grade_id": "cell-efd15bbca288c498",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_candidate_documents_and_scores(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
        "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
        "    Then it will populate the dictionary 'candidates.'\n",
        "    For calculation of IDF, use log with base 10.\n",
        "    tf will be normalized based on the length of the document.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    words,pls: iterator for working with posting.\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of candidates. In the following format:\n",
        "                                                               key: pair (doc_id,term)\n",
        "                                                               value: tfidf score. \n",
        "    \"\"\"\n",
        "    candidates = {}\n",
        "    for term in np.unique(query_to_search):\n",
        "        if term in words:            \n",
        "            list_of_doc = pls[words.index(term)]            \n",
        "            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(len(DL)/index.df[term],10)) for doc_id, freq in list_of_doc]\n",
        "            \n",
        "            for doc_id, tfidf in normlized_tfidf:\n",
        "                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               \n",
        "\n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "11pKF-MqFhAt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "debdd77e8edf03f9341f2a994c1cf339",
          "grade": false,
          "grade_id": "cell-7dc5f8953f2e0523",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_document_tfidf_matrix(query_to_search,index,words,pls):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame `D` of tfidf scores for a given query. \n",
        "    Rows will be the documents candidates for a given query\n",
        "    Columns will be the unique terms in the index.\n",
        "    The value for a given document and term will be its tfidf score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). \n",
        "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
        "\n",
        "    index:           inverted index loaded from the corresponding files.\n",
        "\n",
        "    \n",
        "    words,pls: iterator for working with posting.\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    DataFrame of tfidf scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_vocab_size = len(index.term_total)\n",
        "    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
        "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
        "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
        "    D = pd.DataFrame(D)\n",
        "    \n",
        "    D.index = unique_candidates\n",
        "    D.columns = index.term_total.keys()\n",
        "\n",
        "    for key in candidates_scores:\n",
        "        tfidf = candidates_scores[key]\n",
        "        doc_id, term = key    \n",
        "        D.loc[doc_id][term] = tfidf\n",
        "\n",
        "    return D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk_yJnzM5Tb"
      },
      "source": [
        "`cosine_similarity` -  This function calculate the cosine similarity for each candidate document in D and a given query (e.g., Q) and return a dictionary of cosine similary scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "92OO-c4Ah7TJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5764473e9075707e08292f35a6680eb7",
          "grade": false,
          "grade_id": "cell-b87e721c86d6850d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(D,Q):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "    key: doc_id\n",
        "    value: cosine similarity score\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D: DataFrame of tfidf scores.\n",
        "\n",
        "    Q: vectorized query with tfidf scores\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of cosine similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: cosine similarty score.\n",
        "    \"\"\"\n",
        "    cos_sim_scores_dict = {}\n",
        "    \n",
        "    for cand_doc in D.T.iteritems():\n",
        "\n",
        "      numerator = np.dot(cand_doc[1], Q)\n",
        "    \n",
        "      denominator_for_D = np.power(cand_doc[1], 2)\n",
        "      denominator_for_D = np.sqrt(np.sum(denominator_for_D))\n",
        "  \n",
        "      denominator_for_Q = np.power(Q, 2)\n",
        "      denominator_for_Q = np.sqrt(np.sum(denominator_for_Q))\n",
        "\n",
        "      denominator = denominator_for_D * denominator_for_Q\n",
        "      cos_sim_score = (numerator / denominator)\n",
        "\n",
        "      cos_sim_scores_dict[cand_doc[0]] = cos_sim_score\n",
        "\n",
        "    return cos_sim_scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Yh7H2unw9oTf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1148d91409291dfe74053192176ddff",
          "grade": false,
          "grade_id": "cell-5bad3528aaeb6a2f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_top_n(sim_dict,N=3):\n",
        "    \"\"\" \n",
        "    Sort and return the highest N documents according to the cosine similarity score.\n",
        "    Generate a dictionary of cosine similarity scores \n",
        "   \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sim_dict: a dictionary of similarity score as follows:\n",
        "                                                                key: document id (e.g., doc_id)\n",
        "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
        "\n",
        "    N: Integer (how many documents to retrieve). By default N = 3\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    a ranked list of pairs (doc_id, score) in the length of N.\n",
        "    \"\"\"\n",
        "    \n",
        "    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJUOZwrMTiL"
      },
      "source": [
        "`get_topN_score_for_queries` - This function generate a dictionary that gather for every query its topN score, based on cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "2AJ1qn2YVpN5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1c2ad4ade44cc22dd440bf53b0e6f5d8",
          "grade": false,
          "grade_id": "cell-59c3035e52c567f5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_topN_score_for_queries(queries_to_search,index,N=3):\n",
        "    \"\"\"\n",
        "    Generate a dictionary that gathers for every query its topN score.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    queries_to_search: a dictionary of queries as follows: \n",
        "                                                        key: query_id\n",
        "                                                        value: list of tokens.\n",
        "    index:           inverted index loaded from the corresponding files.    \n",
        "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: a dictionary of queries and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id, score). \n",
        "    \"\"\"\n",
        "\n",
        "    queries_topN_candidates_docs = {}\n",
        "    for query, tokens_of_query in queries_to_search.items():\n",
        "\n",
        "      vectorized_query_tfidf = generate_query_tfidf_vector(tokens_of_query, index)\n",
        "\n",
        "      words, pls = get_posting_iter(index)\n",
        "\n",
        "      tfidf_docs_per_query_matrix = generate_document_tfidf_matrix(tokens_of_query, index, words, pls)\n",
        "\n",
        "      cosine_similarity_matrix = cosine_similarity(tfidf_docs_per_query_matrix, vectorized_query_tfidf)\n",
        "\n",
        "      if N > len(cosine_similarity_matrix): N = len(cosine_similarity_matrix)\n",
        "      top_N_docs_per_query = get_top_n(cosine_similarity_matrix, N)\n",
        "      \n",
        "      queries_topN_candidates_docs[query] = top_N_docs_per_query\n",
        "\n",
        "    return queries_topN_candidates_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this below cell is for tests! later on we will reallocate this to the test class"
      ],
      "metadata": {
        "id": "6h0fuzkYzr0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oxYOsSwMWDmo",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "10bb0b3ec5fdc3580a3f16ff24ebe489",
          "grade": false,
          "grade_id": "cell-9c1d5b562ad2081e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# # this is for us to understand better how to do testings over our cosine similarity\n",
        "# tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this below cell is for tests! later on we will reallocate this to the test class"
      ],
      "metadata": {
        "id": "EE7K0MjDzuHF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7l1WIMatYa"
      },
      "source": [
        "For query 172 we can observe two document with cosine similarity score of 1. Let's have a glance on this query and documents for making sure it makes sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6yB6gHojaN77",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "211e712289f49b0efe56ce97f3db7100",
          "grade": false,
          "grade_id": "cell-1169674deaad07c6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ec82ca-2913-42bc-8e82-fe72032892f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevnt documents and tfidf score for query number 172 : [(320, 1.0), (321, 1.0), (322, 1.0)]\n",
            "query:  ['solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 320:  ['comment', 'improved', 'numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 320:  ['improved', 'numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n",
            "docuemnt 322:  ['numerical', 'solution', 'blasius', 'problem', 'three-point', 'boundary', 'conditions']\n"
          ]
        }
      ],
      "source": [
        "# print('relevnt documents and tfidf score for query number 172 :',tfidf_queries_score_train[172])\n",
        "# print('query: ' ,cran_txt_query_text_train[172])\n",
        "# print('docuemnt 320: ', cran_txt_data_titles['320'])\n",
        "# print('docuemnt 320: ', cran_txt_data_titles['321'])\n",
        "# print('docuemnt 322: ', cran_txt_data_titles['322'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF6VVM9KHC1N"
      },
      "source": [
        "## Task 3: Using weights of title and body scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek3G_fS61s8w"
      },
      "source": [
        "Now we will experiment with two sets of results. \n",
        "The first corresponds to results from the title index. \n",
        "The second corresponds to the results from the body index.\n",
        "\n",
        "We need to merge them into a single result set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9tftQ9WNn0"
      },
      "source": [
        "`merge_results` - \n",
        "This function merge and sort documents retrieved by its weighte score (e.g., title and body).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Oz1yjX5z2MmJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "255498d4034e5373b2c3591fe77e8f47",
          "grade": false,
          "grade_id": "cell-12bfb07aef0d4308",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    \n",
        "    \"\"\"\n",
        "    This function merge and sort documents retrieved by its weighte score (e.g., title and body). \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "                \n",
        "    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n",
        "                                                                            key: query_id\n",
        "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
        "    title_weight: float, for weigted average utilizing title and body scores\n",
        "    text_weight: float, for weigted average utilizing title and body scores\n",
        "    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    dictionary of querires and topN pairs as follows:\n",
        "                                                        key: query_id\n",
        "                                                        value: list of pairs in the following format:(doc_id,score). \n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    finale = {}\n",
        "    for (key_title, doc_term_list_title), (key_body, doc_term_list_body) in zip(title_scores.items(), body_scores.items()):\n",
        "\n",
        "      finale[key_title] = merge_tuples((key_title, doc_term_list_title), (key_body, doc_term_list_body), title_weight, text_weight)[:N]\n",
        "\n",
        "    return finale\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def merge_tuples(tuple_title, tuple_body, t_weight, b_weight):\n",
        "    temporal_dict = {}\n",
        "    list_of_tuples_title = tuple_title[1]\n",
        "    list_of_tuples_body = tuple_body[1]\n",
        "    \n",
        "    size_title = len(list_of_tuples_title)\n",
        "    size_body = len(list_of_tuples_body)\n",
        "\n",
        "    general = []\n",
        "    i, j, position_in_list = 0, 0, 0\n",
        "\n",
        "    while i < size_title and j < size_body:\n",
        "        if list_of_tuples_title[i][1] > list_of_tuples_body[j][1]:\n",
        "            # before_t_weighting = (list_of_tuples_title[i][0], list_of_tuples_title[i][1])\n",
        "            # general.append(before_t_weighting)\n",
        "            if list_of_tuples_title[i][0] in temporal_dict:\n",
        "                temporal_dict[list_of_tuples_title[i][0]] = (temporal_dict[list_of_tuples_title[i][0]][0]+1, temporal_dict[list_of_tuples_title[i][0]][1])\n",
        "            else:\n",
        "                temporal_dict[list_of_tuples_title[i][0]] = (1, position_in_list)\n",
        "                position_in_list += 1\n",
        "\n",
        "            # temporal_dict[list_of_tuples_body[i][0]][1] == (integer, position_in_list)[0] == integer\n",
        "            if temporal_dict[list_of_tuples_title[i][0]][0] > 1:\n",
        "                first = general[temporal_dict[list_of_tuples_title[i][0]][1]][0]\n",
        "                updated_score = general[temporal_dict[list_of_tuples_title[i][0]][1]][1] + list_of_tuples_title[i][1] * t_weight\n",
        "                general[temporal_dict[list_of_tuples_title[i][0]][1]] = (first, updated_score)\n",
        "            else:\n",
        "                after_t_weighting = (list_of_tuples_title[i][0], list_of_tuples_title[i][1] * t_weight)\n",
        "                general.append(after_t_weighting)\n",
        "\n",
        "            i += 1\n",
        "            \n",
        "\n",
        "        else:\n",
        "\n",
        "            if list_of_tuples_body[j][0] in temporal_dict:\n",
        "                temporal_dict[list_of_tuples_body[j][0]] = (temporal_dict[list_of_tuples_body[j][0]][0] + 1, temporal_dict[list_of_tuples_body[j][0]][1])\n",
        "            else:\n",
        "                temporal_dict[list_of_tuples_body[j][0]] = (1, position_in_list)\n",
        "                position_in_list += 1\n",
        "\n",
        "            # temporal_dict[list_of_tuples_body[i][0]][1] == (integer, position_in_list)[0] == integer\n",
        "            if temporal_dict[list_of_tuples_body[j][0]][0] > 1:\n",
        "\n",
        "                first = general[temporal_dict[list_of_tuples_body[j][0]][1]][0]\n",
        "                updated_score = general[temporal_dict[list_of_tuples_body[j][0]][1]][1] + list_of_tuples_body[j][1] * b_weight\n",
        "                general[temporal_dict[list_of_tuples_body[j][0]][1]] = (first, updated_score)\n",
        "\n",
        "            else:\n",
        "                after_b_weighting = (list_of_tuples_body[j][0], list_of_tuples_body[j][1] * b_weight)\n",
        "                general.append(after_b_weighting)\n",
        "\n",
        "            j += 1\n",
        "            \n",
        "\n",
        "\n",
        "    if i >= size_title:\n",
        "        # this while loop append all the rest of the elements that didnwt append to the list in the first iterations\n",
        "        while j < size_body:\n",
        "            if list_of_tuples_body[j][0] in temporal_dict:\n",
        "                temporal_dict[list_of_tuples_body[j][0]] = (temporal_dict[list_of_tuples_body[j][0]][0] + 1, temporal_dict[list_of_tuples_body[j][0]][1])\n",
        "            else:\n",
        "                temporal_dict[list_of_tuples_body[j][0]] = (1, position_in_list)\n",
        "                position_in_list += 1\n",
        "\n",
        "            # temporal_dict[list_of_tuples_body[i][0]][1] == (integer, position_in_list)[0] == integer\n",
        "            if temporal_dict[list_of_tuples_body[j][0]][0] > 1:\n",
        "                first = general[temporal_dict[list_of_tuples_body[j][0]][1]][0]\n",
        "                updated_score = general[temporal_dict[list_of_tuples_body[j][0]][1]][1] + list_of_tuples_body[j][1] * b_weight\n",
        "                # general[temporal_dict[list_of_tuples_body[i][0]][1]]\n",
        "                general[temporal_dict[list_of_tuples_body[j][0]][1]] = (first, updated_score)\n",
        "            else:\n",
        "                after_b_weighting = (list_of_tuples_body[j][0], list_of_tuples_body[j][1] * b_weight)\n",
        "                general.append(after_b_weighting)\n",
        "\n",
        "            j += 1\n",
        "            \n",
        "\n",
        "    elif j >= size_body:\n",
        "        # this while loop append all the rest of the elements that didnwt append to the list in the first iterations\n",
        "        while i < size_title:\n",
        "            if list_of_tuples_title[i][0] in temporal_dict:\n",
        "                temporal_dict[list_of_tuples_title[i][0]] = (temporal_dict[list_of_tuples_title[i][0]][0]+1, temporal_dict[list_of_tuples_title[i][0]][1])\n",
        "            else:\n",
        "                temporal_dict[list_of_tuples_title[i][0]] = (1, position_in_list)\n",
        "                position_in_list += 1\n",
        "\n",
        "            # temporal_dict[list_of_tuples_body[i][0]][1] == (integer, position_in_list)[0] == integer\n",
        "            if temporal_dict[list_of_tuples_title[i][0]][0] > 1:\n",
        "                first = general[temporal_dict[list_of_tuples_title[i][0]][1]][0]\n",
        "                updated_score = general[temporal_dict[list_of_tuples_title[i][0]][1]][1] + list_of_tuples_title[i][1] * t_weight\n",
        "                general[temporal_dict[list_of_tuples_title[i][0]][1]] = (first, updated_score)\n",
        "            else:\n",
        "                after_t_weighting = (list_of_tuples_title[i][0], list_of_tuples_title[i][1]*t_weight)\n",
        "                general.append(after_t_weighting)\n",
        "\n",
        "            i += 1\n",
        "    \n",
        "    general.sort(key=lambda x: x[1], reverse=True)\n",
        "    return general"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cells represent configuration before tests! for later on we need to reallocate them"
      ],
      "metadata": {
        "id": "FQiPsFSLpOEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BPrpxaW1WtPF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "71eef6e9777f134cdb8df964565dae2d",
          "grade": false,
          "grade_id": "cell-eafc1d1e97e87f62",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# #Do Not run this cell for nothing!\n",
        "# bm25_title = BM25_from_index(idx_title)\n",
        "# bm25_body = BM25_from_index(idx_body)\n",
        "\n",
        "# bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)\n",
        "# bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cells represent tests! for later on we need to reallocate them"
      ],
      "metadata": {
        "id": "HGYvZNhkpTwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xHD8GBBZX_RL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7676da02ce4147d45fedb9a118f1fe97",
          "grade": true,
          "grade_id": "cell-78bf3238907c44fa",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# #tests\n",
        "# w1,w2 = 0.5, 0.5\n",
        "# w3,w4 = 0.25,0.75\n",
        "\n",
        "# half_and_half = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,w1,w2)        \n",
        "# assert len(half_and_half[2]) == 3\n",
        "# assert type(half_and_half) == dict\n",
        "# assert type(half_and_half[2]) == list\n",
        "# assert len(half_and_half) == 180\n",
        "# assert half_and_half[2][0][1] == 0.5 * (bm25_queries_score_train_title[2][-1][1]+ bm25_queries_score_train_body[2][0][1])\n",
        "\n",
        "# quarter_and_three_quarters = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,0.25,0.75)        \n",
        "\n",
        "# assert quarter_and_three_quarters[2][0][1] == (w3 * bm25_queries_score_train_title[2][-1][1] + w4 * bm25_queries_score_train_body[2][0][1])\n",
        "# assert {k for k,v in half_and_half[16]} != {k for k,v in quarter_and_three_quarters[16]}\n",
        "# assert len({k for k,v in half_and_half[16]}.union({k for k,v in quarter_and_three_quarters[16]})) < (len({k for k,v in half_and_half[16]}) + len({k for k,v in quarter_and_three_quarters[16]}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finBStQSEfQO"
      },
      "source": [
        "Here we provided three examples of mistakes that the model is making and explanations for why, and describe how we will change the model based on these observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "AyUzoE--JLjq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b41efe1b8a8553eb34c61962061fd7c",
          "grade": true,
          "grade_id": "cell-72de9207dbdfe493",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# 1. Relevance: One mistake that the model might make is returning documents that are not relevant to the query.\n",
        "#    This could be due to the model not accurately understanding the meaning of the query or the content of the documents. \n",
        "#    To improve the model's performance, I can consider using techniques such as query expansion or document summarization to better understand the context and meaning of the query and documents\n",
        "# 2. Precision: Another mistake that the model might make is returning too many documents that are not relevant to the query,\n",
        "#    which can reduce the precision of the search results.\n",
        "#    To improve precision,we can use techniques such as relevance feedback or query refinement to help the model better understand the user's intent.\n",
        "# 3. Recall: Our model might also make the mistake of not returning all of the relevant documents for a given query,\n",
        "#    which can reduce the recall of the search results.\n",
        "#    To improve recall, I can consider using techniques such as pseudorelevance feedback or expanding the search to include additional sources of information. "
      ]
    }
  ]
}