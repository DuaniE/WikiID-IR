{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install notebook --upgrade"
   ],
   "metadata": {
    "id": "biU8yAPGT4aj"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NKr3lcUgwypW"
   },
   "outputs": [],
   "source": [
    "# !jupyter colab --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q pyspark\n",
    "!pip install -U -q PyDrive\n",
    "!apt install openjdk-8-jdk-headless -qq\n",
    "!pip install -q graphframes\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
    "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
    "!wget -N -P $spark_jars $graphframes_jar"
   ],
   "metadata": {
    "id": "zPqzr7ACCdYX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bb856bfd-44ee-4419-dca2-33e9a7f929fa"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m281.4/281.4 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.7/199.7 KB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  openjdk-8-jre-headless\n",
      "Suggested packages:\n",
      "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
      "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
      "0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n",
      "Need to get 36.6 MB of archives.\n",
      "After this operation, 143 MB of additional disk space will be used.\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "(Reading database ... 124016 files and directories currently installed.)\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.7/154.7 KB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h--2023-01-12 17:47:04--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
      "Resolving repos.spark-packages.org (repos.spark-packages.org)... 108.156.83.15, 108.156.83.37, 108.156.83.69, ...\n",
      "Connecting to repos.spark-packages.org (repos.spark-packages.org)|108.156.83.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 247880 (242K) [binary/octet-stream]\n",
      "Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
      "\n",
      "graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2023-01-12 17:47:04 (3.89 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzqD6dfNiRmy",
    "outputId": "73fe2bcd-518c-49de-d48c-9c72c2174772"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# from threading import Thread\n",
    "\n",
    "# import inverted_index_gcp\n",
    "import inverted_index_gcp\n",
    "import math\n",
    "import pickle\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "import time\n",
    "from inverted_index_gcp import MultiFileReader, InvertedIndex\n",
    "import struct\n",
    "import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "# from time import time\n",
    "# from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install a particular version of `google-cloud-storage` because (oddly enough) \n",
    "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
    "!pip install -q google-cloud-storage==1.43.0"
   ],
   "metadata": {
    "id": "vqoqhF-2IwUK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d1cce40e-2350-4c28-cb9a-44e6573b44c2"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/106.6 KB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.6/106.6 KB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# authenticate below for Google Storage access as needed\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n"
   ],
   "metadata": {
    "id": "svKL2qTACpUb"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Copy one wikidumps files \n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import auth\n",
    "## RENAME the project_id to yours project id from the project you created in GCP \n",
    "project_id = 'myfirstgcp-370210'\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "data_bucket_name = 'elad_318640828_titles_bucket'\n",
    "try:\n",
    "    if os.environ[\"elad_318640828_titles_bucket\"] is not None:\n",
    "        pass  \n",
    "except:\n",
    "      !mkdir wikidumps\n",
    "      # !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFEgqPbgC4L4",
    "outputId": "c73cd0fe-f468-4b92-aa57-3848d393885d"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !gsutil cp -R gs://elad_318640828_titles_bucket/small_corpus .\n",
    "!gsutil -m cp -R gs://elad_318640828_titles_bucket/small_postings_gcp ."
   ],
   "metadata": {
    "id": "A4q3MA7_tfS4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ae6ae21-5e93-4656-997e-d64c9ca3ce87"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Copying gs://elad_318640828_titles_bucket/small_postings_gcp/104_posting_locs.pickle...\n",
      "/ [0/8 files][    0.0 B/168.9 MiB]   0% Done                                    \rCopying gs://elad_318640828_titles_bucket/small_postings_gcp/104_000.bin...\n",
      "/ [0/8 files][    0.0 B/168.9 MiB]   0% Done                                    \rCopying gs://elad_318640828_titles_bucket/small_postings_gcp/28_000.bin...\n",
      "/ [0/8 files][    0.0 B/168.9 MiB]   0% Done                                    \rCopying gs://elad_318640828_titles_bucket/small_postings_gcp/56_000.bin...\n",
      "/ [0/8 files][    0.0 B/168.9 MiB]   0% Done                                    \rCopying gs://elad_318640828_titles_bucket/small_postings_gcp/28_posting_locs.pickle...\n",
      "/ [0/8 files][    0.0 B/168.9 MiB]   0% Done                                    \rCopying gs://elad_318640828_titles_bucket/small_postings_gcp/index_title.pkl...\n",
      "Copying gs://elad_318640828_titles_bucket/small_postings_gcp/56_posting_locs.pickle...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Replace <path/to/dictionary.pkl> with the path to your dictionary file\n",
    "with open('./small_postings_gcp/index_title.pkl', 'rb') as f:\n",
    "    index_anchor = pickle.load(f)\n",
    "\n"
   ],
   "metadata": {
    "id": "wcp32_9qX9R0"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(index_anchor.df[\"world\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6Rg6Nkl3wnE",
    "outputId": "0fceee22-19b7-4d7e-9289-10e6f1fa40cc"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS"
   ],
   "metadata": {
    "id": "wmQsCbXHxblT"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TUPLE_SIZE = 6       \n",
    "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
    "from contextlib import closing\n",
    "\n",
    "def read_posting_list(inverted, w):\n",
    "  with closing(MultiFileReader()) as reader:\n",
    "    # print(inverted.posting_locs[w])\n",
    "    locs = inverted.posting_locs[w]\n",
    "    # print(locs)\n",
    "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
    "    # print(b)\n",
    "    posting_list = []\n",
    "    for i in range(inverted.df[w]):\n",
    "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "      posting_list.append((doc_id, tf))\n",
    "    return posting_list"
   ],
   "metadata": {
    "id": "BGNZQcfD1NSq"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"world cUp\"\n",
    "def search_title(query):\n",
    "    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n",
    "        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n",
    "        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n",
    "        tokenization and remove stopwords. \n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "    # query = request.args.get('query', '')\n",
    "    # if len(query) == 0:\n",
    "    #   return jsonify(res)\n",
    "  # BEGIN SOLUTION\n",
    "    dic = {}\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(query.lower())]\n",
    "    for term in tokens:\n",
    "        if index_anchor.df.get(term):\n",
    "            ls_doc_freq = read_posting_list(index_anchor,term)\n",
    "            # print(ls_doc_freq)\n",
    "            for doc, freq in ls_doc_freq:\n",
    "              # print(doc)\n",
    "              dic[doc] = dic.get(doc, 0) + freq\n",
    "    lst_doc = Counter(dic).most_common()\n",
    "    for item in lst_doc:\n",
    "      print(item)\n",
    "      res.append((item[0], index_anchor.id2titles[item[0]]))\n",
    "  # END SOLUTION\n",
    "    return res"
   ],
   "metadata": {
    "id": "g7G3K0aInekI"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "search_title(query)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLMCRwQJcQrE",
    "outputId": "be184def-5a9f-42fb-d75c-cf9f2a48fd5a"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(11370, 2)\n",
      "(16383, 2)\n",
      "(33727, 2)\n",
      "(59707, 2)\n",
      "(168079, 2)\n",
      "(183628, 2)\n",
      "(244862, 2)\n",
      "(656933, 2)\n",
      "(1166428, 2)\n",
      "(1248592, 2)\n",
      "(1853149, 2)\n",
      "(2150801, 2)\n",
      "(2996777, 2)\n",
      "(3482503, 2)\n",
      "(3556431, 2)\n",
      "(4723188, 2)\n",
      "(4743361, 2)\n",
      "(8258172, 2)\n",
      "(8734046, 2)\n",
      "(8821389, 2)\n",
      "(10822574, 2)\n",
      "(13327177, 2)\n",
      "(16842834, 2)\n",
      "(16966712, 2)\n",
      "(17742072, 2)\n",
      "(19537336, 2)\n",
      "(22230053, 2)\n",
      "(26814387, 2)\n",
      "(27007503, 2)\n",
      "(27226732, 2)\n",
      "(29868391, 2)\n",
      "(32352129, 2)\n",
      "(32516422, 2)\n",
      "(36581929, 2)\n",
      "(39302261, 2)\n",
      "(39812824, 2)\n",
      "(41648358, 2)\n",
      "(42931572, 2)\n",
      "(45271353, 2)\n",
      "(51765484, 2)\n",
      "(55490096, 2)\n",
      "(57240806, 2)\n",
      "(57918704, 2)\n",
      "(57918711, 2)\n",
      "(60410401, 2)\n",
      "(60637832, 2)\n",
      "(61269058, 2)\n",
      "(61715824, 2)\n",
      "(62528055, 2)\n",
      "(64467696, 2)\n",
      "(64999764, 2)\n",
      "(64999924, 2)\n",
      "(66040084, 2)\n",
      "(66040086, 2)\n",
      "(67608822, 2)\n",
      "(22888933, 1)\n",
      "(29384326, 1)\n",
      "(36827305, 1)\n",
      "(42072639, 1)\n",
      "(63946361, 1)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(11370, 'FIFA World Cup'),\n",
       " (16383, 'FIFA World Cup Trophy'),\n",
       " (33727, 'World cup'),\n",
       " (59707, '1930 FIFA World Cup'),\n",
       " (168079, '2010 FIFA World Cup'),\n",
       " (183628, 'FIS Alpine Ski World Cup'),\n",
       " (244862, \"FIFA Women's World Cup\"),\n",
       " (656933, '2014 FIFA World Cup'),\n",
       " (1166428, 'FIFA World Cup official mascots'),\n",
       " (1248592, 'FIFA Club World Cup'),\n",
       " (1853149, 'FIFA World Cup video games'),\n",
       " (2150801, '2018 and 2022 FIFA World Cup bids'),\n",
       " (2996777, 'FIFA Futsal World Cup'),\n",
       " (3482503, 'FIFA World Cup awards'),\n",
       " (3556431, 'FIFA World Cup qualification'),\n",
       " (4723188, 'History of the FIFA World Cup'),\n",
       " (4743361, 'FIFA World Cup hosts'),\n",
       " (8258172, \"ICC Men's T20 World Cup\"),\n",
       " (8734046, 'Brazil at the FIFA World Cup'),\n",
       " (8821389, 'List of FIFA World Cup finals'),\n",
       " (10822574, 'List of FIFA World Cup broadcasters'),\n",
       " (13327177, '2018 FIFA World Cup'),\n",
       " (16842834, 'World Cup (disambiguation)'),\n",
       " (16966712, 'Argentina at the FIFA World Cup'),\n",
       " (17742072, '2022 FIFA World Cup'),\n",
       " (19537336, '1930 FIFA World Cup Final'),\n",
       " (22230053, 'Table Tennis World Cup'),\n",
       " (26814387, 'List of FIFA World Cup anthems and songs'),\n",
       " (27007503, 'Japan 2022 FIFA World Cup bid'),\n",
       " (27226732, 'Qatar 2022 FIFA World Cup bid'),\n",
       " (29868391, '2022 FIFA World Cup qualification'),\n",
       " (32352129, 'Mexico at the FIFA World Cup'),\n",
       " (32516422, 'List of FIFA World Cup penalty shoot-outs'),\n",
       " (36581929, '2026 FIFA World Cup'),\n",
       " (39302261, 'Uruguay–Argentina–Chile–Paraguay 2030 FIFA World Cup bid'),\n",
       " (39812824, '2023 Cricket World Cup'),\n",
       " (41648358, 'Cameroon at the FIFA World Cup'),\n",
       " (42931572, '2022 FIFA World Cup controversies'),\n",
       " (45271353, \"2023 FIFA Women's World Cup\"),\n",
       " (51765484, '2022 Under-19 Cricket World Cup'),\n",
       " (55490096, 'Iceland at the FIFA World Cup'),\n",
       " (57240806, \"2022 ICC Men's T20 World Cup\"),\n",
       " (57918704, '2022 FIFA World Cup qualification (UEFA)'),\n",
       " (57918711, '2022 FIFA World Cup qualification (CONMEBOL)'),\n",
       " (60410401, '2022 FIFA World Cup qualification – AFC First Round'),\n",
       " (60637832, '2022 FIFA World Cup qualification – AFC Second Round'),\n",
       " (61269058, 'Qatar at the FIFA World Cup'),\n",
       " (61715824, '2022 FIFA World Cup qualification – CAF Second Round'),\n",
       " (62528055, '2022 FIFA World Cup qualification – UEFA Second Round'),\n",
       " (64467696, '2023 FIFA U-17 World Cup'),\n",
       " (64999764, '2022 FIFA World Cup qualification – CONCACAF First Round'),\n",
       " (64999924, '2022 FIFA World Cup qualification – CONCACAF Third Round'),\n",
       " (66040084, '2022 FIFA World Cup qualification – UEFA Group C'),\n",
       " (66040086, '2022 FIFA World Cup qualification – UEFA Group D'),\n",
       " (67608822, '2022 FIFA World Cup broadcasting rights'),\n",
       " (22888933, 'Give Kids the World Village'),\n",
       " (29384326, 'Spy Kids: All the Time in the World'),\n",
       " (36827305, \"Problem Children Are Coming from Another World, Aren't They?\"),\n",
       " (42072639, 'Kids World (film)'),\n",
       " (63946361, 'Costliest cities in the world')]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "index_anchor.id2titles[38310]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZoLJYXgeqsgL",
    "outputId": "76cd85a1-7556-4d2e-f7fa-8712a70f178f"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Cannabis'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"world cUp\"\n",
    "def search_body(query):\n",
    "    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n",
    "        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n",
    "        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n",
    "        tokenization and remove stopwords. \n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "\n",
    "    with_stop_words_query_tokens = [token.group() for token in RE_WORD.finditer(query.lower())]\n",
    "\n",
    "    query_tokens = []\n",
    "    for tok in with_stop_words_query_tokens:\n",
    "        if tok not in all_stopwords:\n",
    "            query_tokens.append(tok)\n",
    "    query_tokens = Counter(query_tokens)\n",
    "\n",
    "    # this is normaliztion of the query for the cosine similarity\n",
    "    norm_factor = 0\n",
    "    for term, freq in query_tokens.items():\n",
    "      norm_factor += freq**2\n",
    "    norm_factor = math.sqrt(norm_factor)\n",
    "    # print(norm_factor)\n",
    "    if norm_factor == 0:\n",
    "      raise ValueError(\"you insert an empty query\")\n",
    "\n",
    "    sim_dict = {}\n",
    "    for term, freq in query_tokens.items():\n",
    "      try:\n",
    "        posting_list_per_term = read_posting_list(index_body, term)\n",
    "        print(posting_list_per_term)\n",
    "      except:\n",
    "        continue\n",
    "      norm_factor_doc = 0\n",
    "      for doc_id, score in posting_list_per_term:\n",
    "        if doc_id in sim_dict:\n",
    "          sim_dict[doc_id] += score\n",
    "        else:\n",
    "          sim_dict[doc_id] = score\n",
    "      for doc_id in sim_dict.keys():\n",
    "        norm_factor_doc += sim_dict[doc_id]**2\n",
    "      norm_factor_docs_final = math.sqrt(norm_factor_doc)\n",
    "      if norm_factor_docs_final == 0:\n",
    "        raise ValueError(\"You need to check your '.bin' files maybe they are not include in the folder\")\n",
    "    similarity_final_dict = dict(sorted(sim_dict.items(), key= lambda x: (x[1]/(norm_factor * norm_factor_docs_final)), reverse= True))\n",
    "\n",
    "    iteration = 0\n",
    "    for doc_id in similarity_final_dict.keys():\n",
    "      if iteration < 100:\n",
    "        res.append((doc_id, index_body.id2titles[doc_id]))\n",
    "        iteration += 1\n",
    "      break\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "print(search_body(query))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3SSG-iCJJ_1",
    "outputId": "481b4560-e615-45f0-aa0a-55b7335d66ed"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ]
  }
 ]
}
