{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgKHgqbbXpsZ"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# from threading import Thread\n",
    "\n",
    "# import inverted_index_gcp\n",
    "import inverted_index_gcp\n",
    "import math\n",
    "import pickle\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "import time\n",
    "from inverted_index_gcp import MultiFileReader, InvertedIndex\n",
    "import struct\n",
    "import time\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "import pickle as pkl\n",
    "\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Copy one wikidumps files \n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import auth\n",
    "## RENAME the project_id to yours project id from the project you created in GCP \n",
    "project_id = 'myfirstgcp-370210'\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "data_bucket_name = 'elad_318640828'\n",
    "# try:\n",
    "#     if os.environ[\"elad_318640828\"] is not None:\n",
    "#         pass\n",
    "# except:\n",
    "#       !mkdir wikidumps\n",
    "      # !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFEgqPbgC4L4",
    "outputId": "92b40934-276d-4ab5-ab2b-411eda90f6d6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Replace <path/to/dictionary.pkl> with the path to your dictionary file\n",
    "with open(f'{data_bucket_name}/postings_gcp/index_body.pkl', 'rb') as f:\n",
    "    index_body = pickle.load(f)\n",
    "\n"
   ],
   "metadata": {
    "id": "wcp32_9qX9R0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS"
   ],
   "metadata": {
    "id": "wmQsCbXHxblT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TUPLE_SIZE = 6       \n",
    "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
    "from contextlib import closing\n",
    "\n",
    "def read_posting_list(inverted, w):\n",
    "  with closing(MultiFileReader()) as reader:\n",
    "    print(inverted.posting_locs)\n",
    "    locs = inverted.posting_locs[w]\n",
    "    print(locs)\n",
    "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
    "    posting_list = []\n",
    "    for i in range(inverted.df[w]):\n",
    "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "      posting_list.append((doc_id, tf))\n",
    "    return posting_list"
   ],
   "metadata": {
    "id": "BGNZQcfD1NSq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# query = \"world cUp\"\n",
    "def search_body(query):\n",
    "    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n",
    "        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n",
    "        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n",
    "        tokenization and remove stopwords. \n",
    "\n",
    "        To issue a query navigate to a URL like:\n",
    "         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n",
    "        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n",
    "        if you're using ngrok on Colab or your external IP on GCP.\n",
    "    Returns:\n",
    "    --------\n",
    "        list of up to 100 search results, ordered from best to worst where each \n",
    "        element is a tuple (wiki_id, title).\n",
    "    '''\n",
    "    res = []\n",
    "    # query = request.args.get('query', '')\n",
    "    # if len(query) == 0:\n",
    "    #   return jsonify(res)\n",
    "  # BEGIN SOLUTION\n",
    "    with_stop_words_query_tokens = [token.group() for token in RE_WORD.finditer(query.lower())]\n",
    "\n",
    "    query_tokens = []\n",
    "    for tok in with_stop_words_query_tokens:\n",
    "        if tok not in all_stopwords:\n",
    "            query_tokens.append(tok)\n",
    "    query_tokens = Counter(query_tokens)\n",
    "\n",
    "    # this is normaliztion of the query for the cosine similarity\n",
    "    norm_factor = 0\n",
    "    for term, freq in query_tokens.items():\n",
    "      norm_factor += freq**2\n",
    "    norm_factor = math.sqrt(norm_factor)\n",
    "    # print(norm_factor)\n",
    "    if norm_factor == 0:\n",
    "      raise ValueError(\"you insert an empty query\")\n",
    "\n",
    "    sim_dict = {}\n",
    "    for term, freq in query_tokens.items():\n",
    "      try:\n",
    "        posting_list_per_term = read_posting_list(index_body, term)\n",
    "        # print(\"-------\")\n",
    "        # print(posting_list_per_term)\n",
    "        # print(\"-------\")\n",
    "      except:\n",
    "        continue\n",
    "      norm_factor_doc = 0\n",
    "      for doc_id, score in posting_list_per_term:\n",
    "        if doc_id in sim_dict:\n",
    "          sim_dict[doc_id] += score\n",
    "          # print(score)\n",
    "        else:\n",
    "          sim_dict[doc_id] = score\n",
    "      for doc_id in sim_dict.keys():\n",
    "        norm_factor_doc += sim_dict[doc_id]**2\n",
    "      norm_factor_docs_final = math.sqrt(norm_factor_doc)\n",
    "      if norm_factor_docs_final == 0:\n",
    "        raise ValueError(\"You need to check your '.bin' files maybe they are not include in the folder\")\n",
    "    similarity_final_dict = dict(sorted(sim_dict.items(), key= lambda x: (x[1]/(norm_factor * norm_factor_docs_final)), reverse= True))\n",
    "\n",
    "    res = list()\n",
    "    for doc_id in similarity_final_dict.keys():\n",
    "      res.append((doc_id, index_body.id2titles[doc_id]))\n",
    "    print(res)\n",
    "  # END SOLUTION\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "# search_body(query)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3SSG-iCJJ_1",
    "outputId": "77e79659-00e5-4ac3-e5c8-7cd6d115b870"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  }
 ]
}
